{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lithology classification\n",
    "\n",
    "## Data\n",
    "\n",
    "This notebook uses data from the [Upper Condamine Catchment](http://www.bom.gov.au/qld/flood/brochures/condamine_balonne/map_upper.shtml) in the state of Queensland. The data is sourced from personal communication as a project output. It may be shared publicly and downloadable from this sample notebook in the future.\n",
    "\n",
    "![Upper Condamine catchment formations](img/Upper_Condamine_formations.png \"Upper Condamine catchment formations\")\n",
    "\n",
    "(Figure from [this paper](https://www.researchgate.net/figure/Upper-Condamine-catchment-Queensland-Australia-The-Marburg-Subgroup-consists-of_fig1_283184727))\n",
    "\n",
    "## Status\n",
    "\n",
    "As of May 2019 this present document is an output from exploratory work done during an internship by [Sudhir Gupta](https://github.com/Sudhir22).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook compares the performance of two techniques for semi-automated classification . It also summarise work using ontologies for classification for cases where we do not have reliable training sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import geopandas as gpd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only True for co-dev of ela from this use case:\n",
    "ela_from_source = False\n",
    "ela_from_source = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ela_from_source:\n",
    "    if ('ELA_SRC' in os.environ):\n",
    "        root_src_dir = os.environ['ELA_SRC']\n",
    "    elif sys.platform == 'win32':\n",
    "        root_src_dir = r'C:\\Users\\SUD011\\Documents\\pyela-sudhir'\n",
    "    else:\n",
    "        username = os.environ['USER']\n",
    "        root_src_dir = os.path.join('/home', username, 'src/github_jm/pyela')\n",
    "    pkg_src_dir = root_src_dir\n",
    "    sys.path.insert(0, pkg_src_dir)\n",
    "\n",
    "from ela.textproc import *\n",
    "from ela.utils import *\n",
    "from ela.classification import *\n",
    "from ela.visual import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import striplog\n",
    "from striplog import Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('ELA_DATA' in os.environ):\n",
    "    data_path = os.environ['ELA_DATA']\n",
    "elif sys.platform == 'win32':\n",
    "    data_path = r'C:\\data\\Lithology'\n",
    "else:\n",
    "    username = os.environ['USER']\n",
    "    data_path = os.path.join('/home', username, 'data', 'Lithology')\n",
    "\n",
    "condamine_litho_dir = os.path.join(data_path,'Condamine')\n",
    "condamine_litho_xl = os.path.join(condamine_litho_dir, 'MASTER_CONDAMINE_Interpretation_all_combined_Jan2017.xlsx')\n",
    "condamine_litho_pkl = os.path.join(condamine_litho_dir, 'MASTER_CONDAMINE_Interpretation_all_combined_Jan2017.pkl')\n",
    "lexicon_cleaned_pkl = os.path.join(condamine_litho_dir, 'lexicon_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LITHO_DESC_COL='Lithology_original'\n",
    "REGEX_LITHO_CLASS_COL='Regex_lithoclass'\n",
    "# The column name with the simplified lithology as categorised by a hydrogeologist\n",
    "LITHO_CLASS_COL = 'Simplified_lithology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_LITHO_CLASS_COL = 'Simplified_Lithology'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data exploration\n",
    "\n",
    "This first section deliberately covers some of the data discovery process for didactic purposes.\n",
    "\n",
    "## Useful resources\n",
    "\n",
    "Note that some of the terms used for classification are triangulated with [this description](https://www.bioregionalassessments.gov.au/assessments/21-22-data-analysis-clarence-moreton-bioregion/21211-lithological-and-stratigraphic-data)\n",
    "\n",
    "We cache the data in a pickle as it is faster to re-read:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(condamine_litho_pkl):\n",
    "    train_data=pd.read_excel(condamine_litho_xl)\n",
    "    with open(condamine_litho_pkl, 'wb') as handle:\n",
    "        pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(condamine_litho_pkl, 'rb') as handle:\n",
    "        train_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_unprocessed = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train_data[LITHO_CLASS_COL].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We massage the column of simplified lithologies, resulting from a manual classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not obvious from inspection of the the pandas  data frame, but there appears to be NaNs that cause headaches later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(type(x) for x in train_data[LITHO_DESC_COL].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vv = [type(x) is not str for x in df[LITHO_DESC_COL].values]\n",
    "# df.loc[np.array(vv)].head()\n",
    "train_data[LITHO_CLASS_COL] = train_data[LITHO_CLASS_COL].str.lower()\n",
    "train_data[LITHO_DESC_COL] = train_data[LITHO_DESC_COL].replace(np.nan,'',regex=True)\n",
    "train_data[LITHO_DESC_COL] = train_data[LITHO_DESC_COL].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Not obvious upfront but noticed that there are terms where dots and slashes prevent tokenization and lithology term detection. \n",
    "train_data[LITHO_DESC_COL] = v_replace_punctuations(train_data[LITHO_DESC_COL].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(train_data[LITHO_CLASS_COL].values, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the low frequency of some of the classes, we remap them to one of the three main classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[LITHO_CLASS_COL] = train_data[LITHO_CLASS_COL].replace('granite|granodiorite|diorite|basement','bedrock',regex=True)\n",
    "train_data[LITHO_CLASS_COL] = train_data[LITHO_CLASS_COL].replace('gravel','alluvium',regex=False)\n",
    "train_data[LITHO_CLASS_COL] = train_data[LITHO_CLASS_COL].replace('wrong_location|weathering_horizon|tertiary','unknown',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(train_data[LITHO_CLASS_COL].values, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = df[LITHO_DESC_COL]\n",
    "descs = descs.reset_index()\n",
    "descs = descs[LITHO_DESC_COL]\n",
    "descs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a default lexical operation from striplog to expand abbreviations such as 'qtz.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = Lexicon.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(lexicon_cleaned_pkl):\n",
    "    expanded_descs = descs.apply(lex.expand_abbreviations) # takes 2-3 minutes\n",
    "    y = expanded_descs.values\n",
    "    train_data[LITHO_DESC_COL] = expanded_descs\n",
    "    with open(lexicon_cleaned_pkl, 'wb') as handle:\n",
    "        pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(lexicon_cleaned_pkl, 'rb') as handle:\n",
    "        train_data = pickle.load(handle)\n",
    "    y = train_data[LITHO_DESC_COL].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flatten the corpus of words to get the most frequend terms as a guide for which lithology classes we can define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "flat = flat_list_tokens(y)\n",
    "len(set(flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_common = token_freq(flat, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq(df_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condamine_litho_cleaned_pkl = os.path.join(condamine_litho_dir, 'condamine_litho_cleaned.pkl')\n",
    "\n",
    "if not os.path.exists(condamine_litho_cleaned_pkl):\n",
    "    with open(condamine_litho_cleaned_pkl, 'wb') as handle:\n",
    "        pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(condamine_litho_cleaned_pkl, 'rb') as handle:\n",
    "        train_data = pickle.load(handle)\n",
    "        y = train_data[LITHO_DESC_COL]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining lithology classes\n",
    "\n",
    "Starting with the three terms (simplified lithology classes) we want to end up with, we can also add the most frequent terms observed in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litho_class_names = [\"basalt\", \"bedrock\", \"alluvium\", \"unknown\"]\n",
    "\n",
    "lithologies = ['alluvium', 'basalt', 'bedrock', 'clay', 'sandstone','sand','shale','soil','honeycomb','gravel','coal','gravel','silt','soil','rock', 'limestone', 'metal']\n",
    "# see https://www.bioregionalassessments.gov.au/assessments/21-22-data-analysis-clarence-moreton-bioregion/21211-lithological-and-stratigraphic-data\n",
    "# for 'metal' or 'blue metal'\n",
    "any_litho_markers_re = r'alluvium|sand|clay|ston|shale|basa|silt|soil|honey|coal|gravel|rock|mud|metal'\n",
    "\n",
    "regex = re.compile(any_litho_markers_re)\n",
    "\n",
    "lithologies_dict = dict([(x,x) for x in lithologies])\n",
    "lithologies_dict['sands'] = 'sand'\n",
    "lithologies_dict['basalts'] = 'basalt'\n",
    "lithologies_dict['clays'] = 'clay'\n",
    "lithologies_dict['shales'] = 'shale'\n",
    "lithologies_dict['claystone'] = 'clay'\n",
    "lithologies_dict['siltstone'] = 'silt'\n",
    "lithologies_dict['mudstone'] = 'silt' # ??\n",
    "lithologies_dict['capstone'] = 'limestone' # ??\n",
    "lithologies_dict['ironstone'] = 'sandstone' # ??\n",
    "lithologies_dict['topsoil'] = 'soil' # ??\n",
    "\n",
    "lithologies_adjective_dict = {\n",
    "    'sandy' :  'sand',\n",
    "    'clayey' :  'clay',\n",
    "    'clayish' :  'clay',\n",
    "    'shaley' :  'shale',\n",
    "    'silty' :  'ort=Truesilt',\n",
    "    'gravelly' :  'gravel'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the regular expression model on the Condamine dataset\n",
    "\n",
    "We have one first pass at classifying into one of the three target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data[LITHO_DESC_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v_tokens = v_word_tokenize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = v_find_litho_markers(v_tokens, regex=regex)\n",
    "print(summary_regex_tokens(vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_litho = [find_primary_lithology(x, lithologies_dict) for x in vt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(set(prim_litho))\n",
    "plot_freq(token_freq(prim_litho, n_most_common = n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a map from lithology classes down to the simplified classes: alluvium, bedrock, basalt or unknown. We will first do a bit of \"snooping\" on the labelled data; this is admitedly dodgy in a context of comparison of methods, but is in practice necessary to validate some classification assumptions.\n",
    "\n",
    "Given the a priori classification of primary lithologies, what proportions of labels do we have for each primary lithology?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame({ LITHO_CLASS_COL: train_data[LITHO_CLASS_COL], REGEX_LITHO_CLASS_COL: prim_litho, LITHO_DESC_COL: descs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_freq_for_lithology(litho_name, df=tmp_df):\n",
    "    blah = df.loc[tmp_df[REGEX_LITHO_CLASS_COL] == litho_name]\n",
    "    return token_freq(blah[LITHO_CLASS_COL].values, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lithomap = pd.concat([class_freq_for_lithology(litho)['frequency'] for litho in lithologies], axis=1, names=lithologies)\n",
    "# lithomap.columns = lithologies\n",
    "# lithomap.rename(index=dict(zip(list(range(4)), litho_class_names)), inplace=True)\n",
    "# lithomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_classfreq(litho_name, df=tmp_df):\n",
    "    ff = class_freq_for_lithology(litho_name, df=df)\n",
    "    dict(zip(ff['token'],ff['frequency']))\n",
    "    x = pd.DataFrame(dict(zip(ff['token'],np.array(ff['frequency']))), index=[litho_name])\n",
    "    return x\n",
    "\n",
    "def table_litho_mappings(litho_names, df=tmp_df):\n",
    "    return pd.concat( [horizontal_classfreq(x, df=df) for x in litho_names], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_litho_mappings(lithologies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a map from primary lithology terms to the simplified lithologies we have in the labelled data set. \n",
    "Note that to some extent there is some \"snooping\" on the labelled data, above, though we think justifiable in the context using regular expression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_map={\n",
    "    'alluvium' :  'alluvium',\n",
    "    'bedrock' :  'bedrock',\n",
    "    'basalt' :  'basalt',\n",
    "    'metal' :  'basalt',\n",
    "    'honeycomb' :  'basalt',\n",
    "    'clay' :  'alluvium',\n",
    "    'coal' :  'bedrock', \n",
    "    'sandstone' :  'bedrock',\n",
    "    'sand' :  'alluvium',\n",
    "    '' :  'unknown',\n",
    "    'soil' :  'alluvium',\n",
    "    'shale': 'bedrock',\n",
    "    'gravel': 'alluvium',\n",
    "    'silt' : 'bedrock',\n",
    "    'rock' : 'bedrock',\n",
    "    'limestone' : 'alluvium'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prim_litho=list()\n",
    "for x in prim_litho:\n",
    "    final_prim_litho.append(lithology_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(final_prim_litho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_lithology=train_data[LITHO_CLASS_COL].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(modelled, expected):\n",
    "    matches = np.equal(modelled, expected)\n",
    "    return np.count_nonzero(matches)/len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of regex for classifying primary lithologies: \", get_accuracy(final_prim_litho, simplified_lithology))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what lithology descriptions led to an 'unknown' classification to see whether we are missing something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame({ LITHO_CLASS_COL: train_data[LITHO_CLASS_COL], REGEX_LITHO_CLASS_COL: final_prim_litho, LITHO_DESC_COL: descs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_and_sample_df(tmp_df, 'unknown', colname=REGEX_LITHO_CLASS_COL, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unk = tmp_df.loc[ tmp_df[REGEX_LITHO_CLASS_COL] == 'unknown' ]\n",
    "df_unk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = flat_list_tokens(df_unk[LITHO_DESC_COL].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '.join(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(s, title = 'Unclassified via regexp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq(token_freq(flat, n_most_common = 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not use 'loam' as a term. There are others that should be used, but it is unclear how they should be remapped, like \"metal\" and \"file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithologies.append('loam')\n",
    "lithologies.append('granite')\n",
    "lithologies.append('soapstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_litho_markers_re = any_litho_markers_re + '|loam|granite|soap'\n",
    "regex = re.compile(any_litho_markers_re)\n",
    "lithologies_dict['loam'] = 'loam'\n",
    "lithologies_dict['granite'] = 'granite'\n",
    "lithologies_dict['soapstone'] = 'soapstone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_tokens = v_word_tokenize(y)\n",
    "vt = v_find_litho_markers(v_tokens, regex=regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_litho = [find_primary_lithology(x, lithologies_dict) for x in vt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(set(prim_litho))\n",
    "plot_freq(token_freq(prim_litho, n_most_common = n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame({ LITHO_CLASS_COL: train_data[LITHO_CLASS_COL], REGEX_LITHO_CLASS_COL: prim_litho, LITHO_DESC_COL: descs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_litho_mappings(lithologies, df=tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_map['loam'] = 'alluvium'\n",
    "lithology_map['granite'] = 'bedrock'\n",
    "lithology_map['soapstone'] = 'basalt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prim_litho=list()\n",
    "for x in prim_litho:\n",
    "    final_prim_litho.append(lithology_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq(final_prim_litho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame({ LITHO_CLASS_COL: train_data[LITHO_CLASS_COL], REGEX_LITHO_CLASS_COL: final_prim_litho, LITHO_DESC_COL: descs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_and_sample_df(tmp_df, 'unknown', colname=REGEX_LITHO_CLASS_COL, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of regex for classifying primary lithologies: \", get_accuracy(final_prim_litho, simplified_lithology))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame({ LITHO_CLASS_COL: train_data[LITHO_CLASS_COL], REGEX_LITHO_CLASS_COL: final_prim_litho, LITHO_DESC_COL: descs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unk = tmp_df.loc[ tmp_df[REGEX_LITHO_CLASS_COL] == 'unknown' ]\n",
    "df_unk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = flat_list_tokens(df_unk[LITHO_DESC_COL].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '.join(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(s, title = 'Unclassified via regexp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_regex_df( df_unk, '.*basalt.*', LITHO_DESC_COL)\n",
    "# find_regex_df( df_unk, '.*file.*', LITHO_DESC_COL)\n",
    "\n",
    "match_and_sample_df(tmp_df, 'unknown', colname=REGEX_LITHO_CLASS_COL, out_colname=None, size=50, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regex = df.copy()\n",
    "df_regex[REGEX_LITHO_CLASS_COL] = final_prim_litho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condamine_litho_regex = os.path.join(condamine_litho_dir, 'condamine_litho_regex.pkl')\n",
    "with open(condamine_litho_regex, 'wb') as handle:\n",
    "    pickle.dump(df_regex, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the deep learning model on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=train_data_unprocessed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install gensim\n",
    "# conda install tensorflow\n",
    "# conda install keras\n",
    "# pip install wordcloud\n",
    "\n",
    "from ela.experiment.textproc import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dl = train_data.copy()\n",
    "model=Model(train_data_dl,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dl = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE 2019-07-30: reloading cached results as the training takes a fair amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dl = pd.read_csv('prediction_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions_dl is None:\n",
    "    model.initialise_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 2019-07-20: \n",
    "\n",
    "* Training Accuracy: 0.8620\n",
    "* Testing Accuracy:  0.8562\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data.copy()\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix the class to not be so hard coded... \n",
    "x['Description'] = x['Lithology_original']\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if predictions_dl is None: \n",
    "    model.predict(x)\n",
    "    predictions_dl = pd.read_csv('prediction_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of DL for classifying primary lithologies: \", get_accuracy(predictions_dl[DL_LITHO_CLASS_COL], simplified_lithology))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the predictive performance of regexp versus DL\n",
    "\n",
    "* Confusion matrices for RE and DL\n",
    "* Subset each case where DL gets is right and not RE, and get a wordcloud of terms\n",
    "* Subset each case where RE gets is right and not DL, and get a wordcloud of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "condamine_litho_regex = os.path.join(condamine_litho_dir, 'condamine_litho_regex.pkl')\n",
    "with open(condamine_litho_regex, 'rb') as handle:\n",
    "    df_regex = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df_regex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_confusion_matrix(df_regex, colname_true, colname_predicted, labels = litho_class_names):\n",
    "    y_true = df_regex[colname_true].values\n",
    "    y_pred = df_regex[colname_predicted].values    \n",
    "    m = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    return m\n",
    "\n",
    "def normalise_confusion_matrix(cm, axis=1):\n",
    "    return cm.astype('float') / cm.sum(axis=axis)[:, np.newaxis]\n",
    "#     fractions = m / m.astype(np.float).sum(axis=0)\n",
    "#     return fractions\n",
    "\n",
    "def plot_cf_matrix(m, litho_class_names, title='confusion matrix', figsize = (10,7), cmap='cool', center=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(m, index = [i for i in litho_class_names],\n",
    "                      columns = [i for i in litho_class_names])\n",
    "    plt.figure(figsize = figsize)\n",
    "    sn.heatmap(df_cm, annot=True, cmap=cmap, center=center)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('predicted')\n",
    "    plt.ylabel('actual')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.set(font_scale=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_regex = build_confusion_matrix(df_regex, colname_true=LITHO_CLASS_COL, colname_predicted=REGEX_LITHO_CLASS_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m_regex\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.astype('float') / m.sum(axis=0)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([[5,1],[0,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [2,2,2,1,2,1,2,2,1,2,1,1,1,1,2,1]\n",
    "y_pred = [1,2,2,1,2,1,1,2,1,2,1,1,1,1,2,1]\n",
    "\n",
    "m = confusion_matrix(y_true, y_pred, labels=[1,2])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_confusion_matrix(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=False, labels=[1,2], figsize=(7,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_true=y_true, y_pred=y_pred, normalize=True, labels=[1,2], figsize=(7,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions_regex = normalise_confusion_matrix(m_regex)\n",
    "\n",
    "m_dl = build_confusion_matrix(predictions_dl, colname_true=LITHO_CLASS_COL, colname_predicted=DL_LITHO_CLASS_COL)\n",
    "fractions_dl = normalise_confusion_matrix(m_dl)\n",
    "\n",
    "m_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_frac = fractions_dl - fractions_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "print(classification_report(df_regex[LITHO_CLASS_COL].values, df_regex[REGEX_LITHO_CLASS_COL].values, labels=litho_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predictions_dl[LITHO_CLASS_COL].values, predictions_dl[DL_LITHO_CLASS_COL].values, labels=litho_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(m_regex, litho_class_names, title='confusion matrix - Regex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(fractions_regex, litho_class_names, title='confusion matrix - Regex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(df_regex[REGEX_LITHO_CLASS_COL], df_regex[LITHO_CLASS_COL], normalize=True, labels=litho_class_names, figsize=(7,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(m_dl, litho_class_names, title='confusion matrix - DL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(fractions_dl, litho_class_names, title='confusion matrix - DL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(predictions_dl[DL_LITHO_CLASS_COL], predictions_dl[LITHO_CLASS_COL], normalize=True, labels=litho_class_names, figsize=(7,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(diff_frac, litho_class_names, title='', cmap=\"PiYG\", center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cf_matrix(diff_frac.transpose() * 100, litho_class_names, title='DL vs regexp confusion matrix comparison (%)', cmap='bwr', center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-checking the normalisation procedure\n",
    "#true_counts = m.astype(np.float).sum(axis=1) # true preds\n",
    "#f = m / true_counts\n",
    "#f\n",
    "#f = m / m.astype(np.float).sum(axis=0)\n",
    "#f\n",
    "#f.sum(axis=0), f.sum(axis=1)\n",
    "\n",
    "len(df_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a drop with DL in the accuracy predicting basalt as a simplified lithology. Can we get a hint why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions_dl), len(df_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_re = np.logical_and((df_regex.Regex_lithoclass == \"basalt\"), (df_regex.Simplified_lithology == \"basalt\"))\n",
    "successful_dl = np.logical_and((df_regex.Simplified_lithology == \"basalt\"), (predictions_dl[DL_LITHO_CLASS_COL] == \"basalt\"))\n",
    "unsuccessful_dl = np.logical_and((df_regex.Simplified_lithology == \"basalt\"), (predictions_dl[DL_LITHO_CLASS_COL] != \"basalt\"))\n",
    "\n",
    "v = np.logical_and(successful_re, unsuccessful_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_dl_fail = predictions_dl.loc[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = flat_list_tokens(basalt_dl_fail[LITHO_DESC_COL].values)\n",
    "s = ' '.join(flat)\n",
    "show_wordcloud(s, title = 'DL fail on basalt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_dl_fail.sample(n=20, frac=None, replace=False, weights=None, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(basalt_dl_fail), np.sum(successful_re), np.sum(unsuccessful_dl), np.sum(successful_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, when did the NLP/DL method get a false positive on Basalt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#successful_re = np.logical_and((df_regex.Regex_lithoclass == \"basalt\"), (df_regex.Simplified_lithology == \"basalt\"))\n",
    "false_pos_basalt_dl = np.logical_and((df_regex.Simplified_lithology != \"basalt\"), (predictions_dl.Simplified_Lithology == \"basalt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_dl_false_pos = predictions_dl.loc[false_pos_basalt_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = flat_list_tokens(basalt_dl_false_pos[LITHO_DESC_COL].values)\n",
    "s = ' '.join(flat)\n",
    "show_wordcloud(s, title = 'DL False positive on basalt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_dl_false_pos.sample(n=20, frac=None, replace=False, weights=None, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(basalt_dl_false_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the accuracy if geolocation and log descriptions are taken as input features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression for classifying lithologies based on geolocation. Combining the outputs of the logistic regression model and the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_unprocessed.copy()\n",
    "set(train_data['Simplified_lithology'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Simplified_lithology']=train_data['Simplified_lithology'].replace(np.nan,'Unknown',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Simplified_lithology']=train_data['Simplified_lithology'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train_data['Simplified_lithology'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Simplified_lithology'],labels=pd.factorize(train_data['Simplified_lithology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=train_data[['EASTING','NORTHING']][0:len(model.train_X)]\n",
    "test_X=train_data[['EASTING','NORTHING']][len(model.train_X):]\n",
    "train_y=train_data['Simplified_lithology'][0:len(model.train_X)]\n",
    "test_y=train_data['Simplified_lithology'][len(model.train_X):]\n",
    "train_X.replace(np.nan,0.0,inplace=True)\n",
    "test_X.replace(np.nan,0.0,inplace=True)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(C=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data_unprocessed.copy()\n",
    "new_df=pd.DataFrame(train_data[['Lithology_original']].values,columns=['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df[len(model.train_X):].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl=model.predict_certainity(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_pred_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape)\n",
    "print(y_pred_dl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_class_prob=np.mean(np.array([y_pred,y_pred_dl]), axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_class_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_numerical=np.argmax(final_class_prob,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(test_y,final_output_numerical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_lithology_categories=[]\n",
    "for x in final_output_numerical:\n",
    "    simplified_lithology_categories.append(labels[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not increase the accuracy. Don't think the geolocation has an impact on the simplified lithologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides libraries such as RDFLib, OWL 2 for working with ontologies. OWL 2 is better suited for ontology oriented programming since it offers a more pythonic way of managing/creating ontologies.\n",
    "But then, RDFLib works well with .rdf files. Hence, there is a trade-off. Can use based on individual project needs. <br>\n",
    "\n",
    "Protege is an 'IDE' which can be used to manage/create ontologies. It was developed at Stanford and can be downloaded from here [Protege](https://protege.stanford.edu/)<br>\n",
    "Protege can be used to visualise the different relationships defined in an ontology <br>\n",
    "\n",
    "The ontology that I am using can be downloaded from [here](http://ontologydesignpatterns.org/wiki/Ontology:CGI_Simple_Lithology_201001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib.namespace import SKOS\n",
    "from rdflib.namespace import RDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology=rd.Graph()\n",
    "condamine_litho_ontology = os.path.join(condamine_litho_dir, 'SimpleLithology201001.rdf')\n",
    "ontology.parse(condamine_litho_ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_dictionary=dict()\n",
    "for x,y in ontology.subject_objects(SKOS.prefLabel):\n",
    "    lithology_dictionary[x]=y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in ontology.subject_objects(SKOS.broader):\n",
    "    if x in lithology_dictionary.keys() and y in lithology_dictionary.keys():\n",
    "        print(\" broader class of \",lithology_dictionary[x],\" is \",lithology_dictionary[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>These relationships can be converted into fuzzy if-then rules and fed to the machine learning model to make better decisions. <br>\n",
    "Above is only one kind of relationship. We have other relationships such as narrower, description of each label etc. All these combined would make a very \"knowledgeable\" machine learning model. </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing visually\n",
    "\n",
    "Optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and discussions\n",
    "\n",
    "DL much better than regular expressions.<br>\n",
    "\n",
    "Regular expressions use a set of keywords and look for matches in the descriptions. Sometimes, these keywords might not be in the descriptions.<br>\n",
    "\n",
    "Similarly, regular expressions map descriptions to a set of predefined catgories (clay,sand,etc.) which are further refined into broader categories ( alluvium,basalt,bedrock). Geoscientists confirm that the mapping is not one/many-to-one. For example,Clay could be part of alluvium/basalt. This cannot be achieved using the regular expression model.\n",
    "\n",
    "DL, on the other hand, learns the descriptions of different lithology classes. Based on labelled data from geoscientists, DL model learns the different kinds of descriptions which would pertain to for e.g alluvium. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and future work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 ELA",
   "language": "python",
   "name": "ela"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
